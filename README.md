# Description
[MACHINE LEARNING AND PATTERN RECOGNITION](http://www.inf.ed.ac.uk/teaching/courses/mlpr/2016/) - In other words, machine learning from the Bayesian perspective.  So, the primary focus was on generative models. The coursework consisted of two parts and was done in Matlab. The orginal task description can be read [here](./mlpr_assignment.pdf). The report with relevant code pieces is located in [report.pdf](./report.pdf). The final mark was 98/100, the highest mark in the group of more than 50 people.

## First Task
It was a regression task where we were asked to predict a missing target pixel value in the bottom of some tissue images. We compared two models: Neural Network (simple MLP) and linear regression. The results for NN and linear regression were quite close, that is why I chose the linear regression as my primal model, as it was a simpler model (Occam's razor principle). The only useful insight about dataset was the correlation of intensities between the target pixel and pixels around it. Thus, I determined an effective radius of pixels that correlated mostly with the target pixel via cross-validation. Once it was done I got one of the lowest root-mean-square errors (RMSE) in the group.

## Second Task
We learnt the basic usage of hierarchical models and MCMC on a binary text classification task. In 2.1c I noticed that a simple linear classifier should have perfectly separated a dataset with less than 100 examples considering that dimensionality of data was more than 100 dimensions unless there was a problem with the dataset. Indeed, there were cases of mislabels when 2 or more datapoints had absolutely identical features but different labels. In 2.2 it was interesting to see how accounting for data corruption via proper custom prior helped to improve overall classification results.
